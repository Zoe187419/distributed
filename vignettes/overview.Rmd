---
title: "Overview of the distributed package"
author: "Kazuki Yoshida"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
runtime: shiny
---

```{r, message = FALSE, tidy = FALSE, echo = F}
## knitr configuration: http://yihui.name/knitr/options#chunk_options
library(knitr)
showMessage <- FALSE
showWarning <- TRUE
set_alias(w = "fig.width", h = "fig.height", res = "results")
opts_chunk$set(comment = "##", error= TRUE, warning = showWarning, message = showMessage,
               tidy = FALSE, cache = F, echo = T, eval = FALSE,
               fig.width = 7, fig.height = 7, dev.args = list(family = "sans"))
## R configuration
options(width = 116, scipen = 5)
```

## Introduction

In distributed data networks such as the Sentinel and PCORnet, minimizing the amount of data shared across data partners is important for reducing the danger of potential privacy breach. In this project, we examined the performance of data analysis methods at various levels of data sharing such as meta-analysis, summary table data, risk set data, and individual-level data.


## System Requirement

Running the entire simulation requires working installation of R as well as UNIX SAS that can be called via sas command. Most part of the simulation is in pure R, but the experimental weighted risk set analysis is implemented in SAS. When the sas command is not found, this part of simulation is skipped gracefully.


## Installation

The package can be installed as follows. If it asks for dependencies, you may need to install these required packages first.

```
R CMD install distributed_0.1.0.tar.gz
```

## Overview of Package Contents

The distributed package contains the functions used to generate, prepare, and analyze data. It also contains script files that are used to run the simulation study.

The functions can be loaded in R using `r library(distributed)`. The scripts are found in the package folder as `scripts.zip`. For example, in the latest R on macOS, it is found in the following path. Please copy this file somewhere convenient and unarchive.

```
/Library/Frameworks/R.framework/Versions/3.3/Resources/library/distributed/scripts.zip
```

The unarchived folder contains the following R and shell scripts as well as subfolders.

```
./scripts
├── 01.GenerateData.R
├── 01.GenerateData_Lsf.sh
├── 01.GenerateData_Slurm.sh
├── 02.PrepareData.R
├── 02.PrepareData_Lsf.sh
├── 02.PrepareData_Slurm.sh
├── 03.AnalyzeData.R
├── 03.AnalyzeData_Lsf.sh
├── 03.AnalyzeData_Slurm.sh
├── 04.AssessMethods.R
├── data
├── log
├── log_lsf
├── log_slurm
└── summary
```


## Running Simulation

The simulation has four distinct phases as evident from the numbering of script file names.

- Data Generation
- Data Preparation
- Data Analysis
- Method Assessment


### Data Generation

Running the following will generate files containing simulated distributed data network under the data subfolder. The 4 at the end of the command specifies the number of CPU cores to use.

```
Rscript 01.GenerateData.R 4
```

Multiple files are generated for each scenario to lessen the resource requirement. Each new file generated under the data subfolder has a name such as `ScenarioRaw001_part001_R50.RData`, where first number indicates the scenario, part number indicates which part it is in a series of files under this scenario, and R50 indicates the number of iterations included in the file.

If you are running the simulation in a Linux cluster environment with job managers, the following scripts can aid dispatching the data generation job to a node. These shell scripts are designed for the Harvard University's Odyssey cluster (SLURM job manager) and Harvard Medical School's Orchestra cluster (LSF). Thus, these script will require modification according to the configuration of the cluster system you are using.

For a LSF-based system:
```
sh 01.GenerateData_Lsf.sh
```

For a SLURM-based system:
```
sh 01.GenerateData_Slurm.sh
```


### Data Preparation

This step fits summary score models in the data and performs matching, stratification, and weighting by these estimated summary scores. Conceptually, this part corresponds to what each site does in a distributed data network. The process has to be run on each data file as follows. The 4 at the end of the command specifies the number of CPU cores to use.

```
Rscript 02.PrepareData.R ./data/ScenarioRaw001_part001_R50.RData 4
```

This will generate a new file named `ScenarioPrepared001_part001_R50.RData` under the data subfolder. This process can be repeated for each file via for loop, but it is better suited for a cluster system. The following scripts dispatch the data preparation job on each file to a separate node, thereby, allowing highly parallel execution. Again the files included are specialized for the clusters the authors used, and need modification before use at a different system.

For a LSF-based system:
```
sh 02.PrepareData_Lsf.sh ./data/ScenarioRaw*
```

For a SLURM-based system:
```
sh 02.PrepareData_Slurm.sh ./data/ScenarioRaw*
```


### Data Analysis

This step conducts the actual analysis of prepared data for the treatment effect of interest. The process has to be run on each data file as follows. The 4 at the end of the command specifies the number of CPU cores to use.

```
Rscript 03.AnalyzeData.R ./data/ScenarioPrepared001_part001_R50.RData 4
```

This will generate a new file named `ScenarioAnalyzed001_part001_R50.RData` under the data subfolder. Again this can be repeated using a for loop or dispatched to multiple nodes in a cluster system.

For a LSF-based system:
```
sh 03.AnalyzeData_Lsf.sh ./data/ScenarioPrepared*
```

For a SLURM-based system:
```
sh 03.AnalyzeData_Slurm.sh ./data/ScenarioPrepared*
```


### Method Assessment

This step aggregates the analysis results and examine performance metrics for each method. The following will load all data files with names containing `ScenarioAnalyzed` (analysis result files), and output assessment results in the summary subfolder.

```
Rscript 04.AssessMethods.R
```

This part is not so computationally intensive, so it can be conducted on a local computer easily even if you are using a cluster for other steps.

----
